# æœ¬åœ°ç§æœ‰åŒ–/å¤§æ¨¡å‹æ¥å£æ¥å…¥

ä¾æ‰˜äºå¼€æºçš„ LLM ä¸ Embedding æ¨¡å‹ï¼Œæœ¬é¡¹ç›®å¯å®ç°åŸºäºå¼€æºæ¨¡å‹çš„ç¦»çº¿ç§æœ‰éƒ¨ç½²ã€‚æ­¤å¤–ï¼Œæœ¬é¡¹ç›®ä¹Ÿæ”¯æŒ OpenAI API çš„è°ƒç”¨ã€‚

## ğŸ“œ ç›®å½•
- [ æœ¬åœ°ç§æœ‰åŒ–æ¨¡å‹æ¥å…¥](#æœ¬åœ°ç§æœ‰åŒ–æ¨¡å‹æ¥å…¥)
- [ å…¬å¼€å¤§æ¨¡å‹æ¥å£æ¥å…¥](#å…¬å¼€å¤§æ¨¡å‹æ¥å£æ¥å…¥)
- [ å¯åŠ¨å¤§æ¨¡å‹æœåŠ¡](#å¯åŠ¨å¤§æ¨¡å‹æœåŠ¡)

## æœ¬åœ°ç§æœ‰åŒ–æ¨¡å‹æ¥å…¥

<br>æ¨¡å‹åœ°å€é…ç½®ç¤ºä¾‹ï¼Œmodel_config.pyé…ç½®ä¿®æ”¹

```bash
# å»ºè®®ï¼šèµ°huggingfaceæ¥å…¥ï¼Œå°½é‡ä½¿ç”¨chatæ¨¡å‹ï¼Œä¸è¦ä½¿ç”¨baseï¼Œæ— æ³•è·å–æ­£ç¡®è¾“å‡º
# æ³¨æ„ï¼šå½“llm_model_dictå’ŒVLLM_MODEL_DICTåŒæ—¶å­˜åœ¨æ—¶ï¼Œä¼˜å…ˆå¯åŠ¨VLLM_MODEL_DICTä¸­çš„æ¨¡å‹é…ç½®

# llm_model_dict é…ç½®æ¥å…¥ç¤ºä¾‹å¦‚ä¸‹
llm_model_dict = {
    "chatglm-6b": {
        "local_model_path": "THUDM/chatglm-6b",
        "api_base_url": "http://localhost:8888/v1",  # "name"ä¿®æ”¹ä¸ºfastchatæœåŠ¡ä¸­çš„"api_base_url"
        "api_key": "EMPTY"
    }
}

# VLLM_MODEL_DICT é…ç½®æ¥å…¥ç¤ºä¾‹å¦‚ä¸‹
VLLM_MODEL_DICT = {
 'chatglm2-6b':  "THUDM/chatglm-6b",
}

```

<br>æ¨¡å‹è·¯å¾„å¡«å†™ç¤ºä¾‹

```bash
# 1ã€è‹¥æŠŠæ¨¡å‹æ”¾åˆ° ~/codefuse-chatbot/llm_models è·¯å¾„ä¸‹
# è‹¥æ¨¡å‹åœ°å€å¦‚ä¸‹
model_dir: ~/codefuse-chatbot/llm_models/THUDM/chatglm-6b

# å‚è€ƒé…ç½®å¦‚ä¸‹
llm_model_dict = {
    "chatglm-6b": {
        "local_model_path": "THUDM/chatglm-6b",
        "api_base_url": "http://localhost:8888/v1",  # "name"ä¿®æ”¹ä¸ºfastchatæœåŠ¡ä¸­çš„"api_base_url"
        "api_key": "EMPTY"
    }
}

VLLM_MODEL_DICT = {
 'chatglm2-6b':  "THUDM/chatglm-6b",
}

# or è‹¥æ¨¡å‹åœ°å€å¦‚ä¸‹
model_dir: ~/codefuse-chatbot/llm_models/chatglm-6b
llm_model_dict = {
    "chatglm-6b": {
        "local_model_path": "chatglm-6b",
        "api_base_url": "http://localhost:8888/v1",  # "name"ä¿®æ”¹ä¸ºfastchatæœåŠ¡ä¸­çš„"api_base_url"
        "api_key": "EMPTY"
    }
}

VLLM_MODEL_DICT = {
 'chatglm2-6b':  "chatglm-6b",
}

# 2ã€è‹¥ä¸æƒ³ç§»åŠ¨ç›¸å…³æ¨¡å‹åˆ° ~/codefuse-chatbot/llm_models
# åŒæ—¶åˆ é™¤ `æ¨¡å‹è·¯å¾„é‡ç½®` ä»¥ä¸‹çš„ç›¸å…³ä»£ç ï¼Œå…·ä½“è§model_config.py
# è‹¥æ¨¡å‹åœ°å€å¦‚ä¸‹
model_dir: ~/THUDM/chatglm-6b
# å‚è€ƒé…ç½®å¦‚ä¸‹
llm_model_dict = {
    "chatglm-6b": {
        "local_model_path": "~/THUDM/chatglm-6b",
        "api_base_url": "http://localhost:8888/v1",  # "name"ä¿®æ”¹ä¸ºfastchatæœåŠ¡ä¸­çš„"api_base_url"
        "api_key": "EMPTY"
    }
}

VLLM_MODEL_DICT = {
 'chatglm2-6b':  "~/THUDM/chatglm-6b",
}
```

```bash
# 3ã€æŒ‡å®šå¯åŠ¨çš„æ¨¡å‹æœåŠ¡ï¼Œä¸¤è€…ä¿æŒä¸€è‡´
LLM_MODEL = "gpt-3.5-turbo-16k"
LLM_MODELs = ["gpt-3.5-turbo-16k"]
```

```bash
# server_config.pyé…ç½®ä¿®æ”¹ï¼Œ è‹¥LLM_MODELSæ— å¤šä¸ªæ¨¡å‹é…ç½®ä¸éœ€è¦é¢å¤–è¿›è¡Œè®¾ç½®
# ä¿®æ”¹server_config.py#FSCHAT_MODEL_WORKERSçš„é…ç½®
"model_name": {'host': DEFAULT_BIND_HOST, 'port': 20057}
```



<br>é‡åŒ–æ¨¡å‹æ¥å…¥

```bash
# è‹¥éœ€è¦æ”¯æ’‘codellama-34b-int4æ¨¡å‹ï¼Œéœ€è¦ç»™fastchatæ‰“ä¸€ä¸ªè¡¥ä¸
cp examples/gptq.py ~/site-packages/fastchat/modules/gptq.py

# è‹¥éœ€è¦æ”¯æ’‘qwen-72b-int4æ¨¡å‹ï¼Œéœ€è¦ç»™fastchatæ‰“ä¸€ä¸ªè¡¥ä¸
cp examples/gptq.py ~/site-packages/fastchat/modules/gptq.py
# é‡åŒ–éœ€ä¿®æ”¹llm_api.pyçš„é…ç½®
# dev_opsgpt/service/llm_api.py#559 å–æ¶ˆæ³¨é‡Š kwargs["gptq_wbits"] = 4
```

## å…¬å¼€å¤§æ¨¡å‹æ¥å£æ¥å…¥

```bash
# model_config.pyé…ç½®ä¿®æ”¹
# ONLINE_LLM_MODEL
# å…¶å®ƒæ¥å£å¼€å‘æ¥è‡ªäºlangchain-chatchaté¡¹ç›®ï¼Œç¼ºå°‘ç›¸å…³è´¦å·æœªç»æµ‹è¯•

# æŒ‡å®šå¯åŠ¨çš„æ¨¡å‹æœåŠ¡ï¼Œä¸¤è€…ä¿æŒä¸€è‡´
LLM_MODEL = "gpt-3.5-turbo-16k"
LLM_MODELs = ["gpt-3.5-turbo-16k"]
```

å¤–éƒ¨å¤§æ¨¡å‹æ¥å£æ¥å…¥ç¤ºä¾‹

```bash
# 1ã€å®ç°æ–°çš„æ¨¡å‹æ¥å…¥ç±»
# å‚è€ƒ  ~/dev_opsgpt/service/model_workers/openai.py#ExampleWorker
# å®ç°do_chatå‡½æ•°å³å¯ä½¿ç”¨LLMçš„èƒ½åŠ›

class XXWorker(ApiModelWorker):
    def __init__(
            self,
            *,
            controller_addr: str = None,
            worker_addr: str = None,
            model_names: List[str] = ["gpt-3.5-turbo"],
            version: str = "gpt-3.5",
            **kwargs,
    ):
        kwargs.update(model_names=model_names, controller_addr=controller_addr, worker_addr=worker_addr)
        kwargs.setdefault("context_len", 16384) #TODO 16Kæ¨¡å‹éœ€è¦æ”¹æˆ16384
        super().__init__(**kwargs)
        self.version = version

    def do_chat(self, params: ApiChatParams) -> Dict:
        '''
        æ‰§è¡ŒChatçš„æ–¹æ³•ï¼Œé»˜è®¤ä½¿ç”¨æ¨¡å—é‡Œé¢çš„chatå‡½æ•°ã€‚
        :params.messages : [
            {"role": "user", "content": "hello"}, 
            {"role": "assistant", "content": "hello"}
            ]
        :params.xx: è¯¦æƒ…è§ ApiChatParams 
        è¦æ±‚è¿”å›å½¢å¼ï¼š{"error_code": int, "text": str}
        '''
        return {"error_code": 500, "text": f"{self.model_names[0]}æœªå®ç°chatåŠŸèƒ½"}


# æœ€ååœ¨ ~/dev_opsgpt/service/model_workers/__init__.py ä¸­å®Œæˆæ³¨å†Œ
# from .xx import XXWorker

# 2ã€é€šè¿‡å·²æœ‰æ¨¡å‹æ¥å…¥ç±»å®Œæˆæ¥å…¥
# æˆ–è€…ç›´æ¥ä½¿ç”¨å·²æœ‰çš„ç›¸å…³å¤§æ¨¡å‹ç±»è¿›è¡Œä½¿ç”¨ï¼ˆç¼ºå°‘ç›¸å…³è´¦å·æµ‹è¯•ï¼Œæ¬¢è¿å¤§å®¶æµ‹è¯•åæPRï¼‰
```


```bash
# model_config.py#ONLINE_LLM_MODEL é…ç½®ä¿®æ”¹
# å¡«å†™ä¸“å±æ¨¡å‹çš„ versionã€api_base_urlã€api_keyã€providerï¼ˆä¸ä¸Šè¿°ç±»åä¸€è‡´ï¼‰
ONLINE_LLM_MODEL = {
    # çº¿ä¸Šæ¨¡å‹ã€‚è¯·åœ¨server_configä¸­ä¸ºæ¯ä¸ªåœ¨çº¿APIè®¾ç½®ä¸åŒçš„ç«¯å£

    "openai-api": {
        "model_name": "gpt-3.5-turbo",
        "api_base_url": "https://api.openai.com/v1",
        "api_key": "",
        "openai_proxy": "",
    },
    "example": {
        "version": "gpt-3.5",  # é‡‡ç”¨openaiæ¥å£åšç¤ºä¾‹
        "api_base_url": "https://api.openai.com/v1",
        "api_key": "",
        "provider": "ExampleWorker",
    },
}
```

## å¯åŠ¨å¤§æ¨¡å‹æœåŠ¡
```bash
# start llm-serviceï¼ˆå¯é€‰ï¼‰  å•ç‹¬å¯åŠ¨å¤§æ¨¡å‹æœåŠ¡
python dev_opsgpt/service/llm_api.py
```

```bash
# å¯åŠ¨æµ‹è¯•
import openai
# openai.api_key = "EMPTY" # Not support yet
openai.api_base = "http://127.0.0.1:8888/v1"

# é€‰æ‹©ä½ å¯åŠ¨çš„æ¨¡å‹
model = "example"

# create a chat completion
completion = openai.ChatCompletion.create(
    model=model,
    messages=[{"role": "user", "content": "Hello! What is your name? "}],
    max_tokens=100,
)
# print the completion
print(completion.choices[0].message.content)

# æ­£ç¡®è¾“å‡ºååˆ™ç¡®è®¤LLMå¯æ­£å¸¸æ¥å…¥
```



or

```bash
# model_config.py#USE_FASTCHAT åˆ¤æ–­æ˜¯å¦è¿›è¡Œfastchatæ¥å…¥æœ¬åœ°æ¨¡å‹
USE_FASTCHAT = "gpt" not in LLM_MODEL
python start.py #224 è‡ªåŠ¨æ‰§è¡Œ python service/llm_api.py
```